import argparse
import json
import os
import time
from typing import List, TypedDict

from langgraph.graph import END, START, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from openai import OpenAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from tavily import TavilyClient


class SearchState(TypedDict, total=False):
    """çŠ¶æ€å®šä¹‰ï¼ŒåŒ…å«å¯¹è¯æ¶ˆæ¯å’Œå„é˜¶æ®µäº§ç‰©ã€‚"""

    messages: List[HumanMessage | AIMessage]
    user_query: str
    search_query: str | None
    search_results: str | None
    step: str | None
    final_answer: str | None
    time_span: dict | None
    interest: str | None
    subjects: List[str] | None


def _get_llm() -> OpenAI:
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        raise RuntimeError("DEEPSEEK_API_KEY æœªè®¾ç½®ï¼Œæ— æ³•è°ƒç”¨ LLM")
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


def llm_node(state: SearchState) -> dict:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯ã€‚"""
    user_message = state["messages"][-1].content
    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆï¼ˆå…³æ³¨æ—¶é—´çº¿ã€å…³å¿ƒçš„äº‹ä»¶ã€æ¶‰åŠä¸»ä½“ï¼‰
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼Œæ–¹ä¾¿æŸ¥åˆ°æ—¶é—´/ä¸»ä½“ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    client = _get_llm()
    resp = client.chat.completions.create(
        model=os.environ.get("DEEPSEEK_MODEL", "deepseek-chat"),
        messages=[{"role": "system", "content": understand_prompt}],
        temperature=0.0,
    )
    response_text = resp.choices[0].message.content if resp.choices else ""

    search_query = user_message  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š", 1)[1].strip()

    return {
        "user_query": user_message,
        "search_query": search_query,
        "step": "understood",
        "messages": state["messages"] + [AIMessage(content=f"æˆ‘å°†ä¸ºæ‚¨æœç´¢ï¼š{search_query}")],
    }


def _format_tavily_results(raw: dict) -> str:
    answer = raw.get("answer")
    hits = raw.get("results", [])
    lines = []
    if answer:
        lines.append(f"å›ç­”æ‘˜è¦ï¼š{answer}")
    for item in hits:
        title = item.get("title") or "æœªå‘½åç»“æœ"
        url = item.get("url") or ""
        content = item.get("content") or ""
        lines.append(f"- {title} ({url})\n  {content}")
    return "\n".join(lines) if lines else "æœªè·å–åˆ°æœç´¢ç»“æœ"


def web_node(state: SearchState) -> dict:
    """æ­¥éª¤2ï¼šä½¿ç”¨ Tavily API è¿›è¡ŒçœŸå®æœç´¢ï¼ˆæ—  Key æ—¶è‡ªåŠ¨è·³è¿‡ï¼‰ã€‚"""
    search_query = state.get("search_query") or state["user_query"]
    api_key = os.getenv("TAVILY_API_KEY")
    if not api_key:
        return {
            "search_results": "æœªé…ç½® TAVILY_API_KEYï¼Œè·³è¿‡çœŸå®æœç´¢ã€‚",
            "step": "search_failed",
            "messages": state["messages"] + [AIMessage(content="æœªé…ç½® Tavilyï¼Œæ”¹ä¸ºç›´æ¥å›ç­”ã€‚")],
        }

    client = TavilyClient(api_key=api_key)
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        response = client.search(
            query=search_query, search_depth="basic", max_results=5, include_answer=True
        )
        search_results = _format_tavily_results(response)
        return {
            "search_results": search_results,
            "step": "searched",
            "messages": state["messages"] + [AIMessage(content="âœ… æœç´¢å®Œæˆï¼æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")],
        }
    except Exception as e:
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{e}",
            "step": "search_failed",
            "messages": state["messages"] + [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜ï¼Œæ”¹ä¸ºç›´æ¥å›ç­”ã€‚")],
        }


def answer_node(state: SearchState) -> dict:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆç»“æ„åŒ–ä¸‰é¡¹ç­”æ¡ˆã€‚"""
    client = _get_llm()
    if state["step"] == "search_failed":
        prompt = (
            "æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ä»…åŸºäºå·²çŸ¥å¸¸è¯†å›ç­”ï¼Œè‹¥ä¸ç¡®å®šè¯·å†™ unknownï¼Œ"
            "åŒæ—¶åœ¨ note ä¸­è¯´æ˜ä¾æ®ä¸è¶³ã€‚\n"
            f"ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}"
        )
    else:
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œå›ç­”ä¸‰ä¸ªé—®é¢˜ï¼Œè‹¥ä¸ç¡®å®šè¯·è¾“å‡º unknown å¹¶é™„ noteï¼š
ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}
æœç´¢ç»“æœï¼š
{state['search_results']}

è¾“å‡º JSONï¼Œæ ¼å¼ï¼š
{{
  "time_span": {{"start": "YYYY-MM-DD", "end": "YYYY-MM-DD", "note": "ä¸ç¡®å®šæ—¶è¯´æ˜æ¨æ–­ä¾æ®"}},
  "interest": "ç”¨æˆ·æœ€å…³å¿ƒçš„å…·ä½“äº‹ä»¶/é—®é¢˜",
  "subjects": ["å—å½±å“çš„ä¸»ä½“1", "ä¸»ä½“2"]
}}
è¦æ±‚ï¼š
- start/end å°½é‡ç²¾ç¡®åˆ°æ—¥ï¼›æ— æ³•ç²¾ç¡®åˆ™ç»™å‡ºæœ€åˆç†æ—¥æœŸæˆ– unknownã€‚
- subjects ç»™å‡º12ä¸ªï¼Œä½¿ç”¨ç®€çŸ­åç§°ï¼ŒæŒ‰ç›¸å…³æ€§æ’åºã€‚"""

    resp = client.chat.completions.create(
        model=os.environ.get("DEEPSEEK_MODEL", "deepseek-chat"),
        messages=[{"role": "system", "content": prompt}],
        temperature=0.2,
    )
    answer = resp.choices[0].message.content if resp.choices else ""
    try:
        parsed = json.loads(answer)
    except Exception:
        parsed = {}

    return {
        "final_answer": answer,
        "time_span": parsed.get("time_span"),
        "interest": parsed.get("interest"),
        "subjects": parsed.get("subjects"),
        "step": "completed",
        "messages": state["messages"] + [AIMessage(content=answer)],
    }


def build_graph():
    """æ„å»º LangGraph å·¥ä½œæµå›¾ã€‚"""
    workflow = StateGraph(SearchState)

    workflow.add_node("llm_node", llm_node)
    workflow.add_node("web_node", web_node)
    workflow.add_node("answer_node", answer_node)

    workflow.add_edge(START, "llm_node")
    workflow.add_edge("llm_node", "web_node")
    workflow.add_edge("web_node", "answer_node")
    workflow.add_edge("answer_node", END)

    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app


def run_workflow(question: str) -> dict:
    """å¯¹å¤–è°ƒç”¨å·¥ä½œæµï¼Œè¿”å›å®Œæ•´çŠ¶æ€ã€‚"""
    app = build_graph()
    final_state = app.invoke(
        {
            "messages": [HumanMessage(content=question)],
            "user_query": question,
            "search_query": None,
            "search_results": None,
            "step": None,
            "final_answer": None,
            "time_span": None,
            "interest": None,
            "subjects": None,
        },
        config={"configurable": {"thread_id": f"cli-{int(time.time())}"}},
    )
    return final_state


def run_once(question: str) -> str:
    final_state = run_workflow(question)
    return final_state.get("final_answer", "")


def main():
    parser = argparse.ArgumentParser(description="ç®€å•çš„æœç´¢-å›ç­”å·¥ä½œæµæ¼”ç¤º")
    parser.add_argument("query", nargs="?", help="è¦æé—®çš„å†…å®¹ï¼ˆä¸ºç©ºåˆ™è¿›å…¥äº¤äº’è¾“å…¥ï¼‰")
    args = parser.parse_args()

    question = args.query or input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š").strip()
    if not question:
        raise SystemExit("é—®é¢˜ä¸èƒ½ä¸ºç©º")

    answer = run_once(question)
    print("\n=== å›ç­” ===")
    print(answer)


if __name__ == "__main__":
    main()
